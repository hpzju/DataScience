{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Evns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pwd': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience',\n",
       " 'datapath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data',\n",
       " 'infilename': 'dictionary.json',\n",
       " 'outfilename': 'etl_dictionary.json',\n",
       " 'infilepath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data\\\\dictionary.json',\n",
       " 'outfilepath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data\\\\etl_dictionary.json'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "envs = {\n",
    "    'pwd': os.getcwd(),\n",
    "    'datapath': os.path.join(os.getcwd(), 'data'),\n",
    "    'infilename': r'dictionary.json',\n",
    "    'outfilename': r'etl_dictionary.json',\n",
    "}\n",
    "envs['infilepath'] =  os.path.join(envs['datapath'], envs['infilename'])\n",
    "envs['outfilepath'] = os.path.join(envs['datapath'], envs['outfilename'])\n",
    "envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 49536\n",
      "sizeof \"dictionary.json\": 2.5 MB\n",
      "sizeof \"etl_dictionary.json\": 4.67e-05 MB\n"
     ]
    }
   ],
   "source": [
    "inputfile = envs['infilepath']\n",
    "outputfile = envs['outfilepath']\n",
    "with open(inputfile, 'r') as ifo, open(outputfile, 'w+') as ofo:\n",
    "    indata = json.load(ifo)\n",
    "    for count, (k, v) in enumerate(indata.items()):\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Total lines: {count}')\n",
    "    json.dump(indata, ofo, indent=4, separators=(', ', ': '))\n",
    "\n",
    "    ofo.seek(0)\n",
    "    outdata = ofo.read()\n",
    "    print(f'''sizeof \"{envs['infilename']}\": {sys.getsizeof(indata)/1024/1024:.3} MB''')\n",
    "    print(f'''sizeof \"{envs['outfilename']}\": {sys.getsizeof(outdata)/1024/1024:.3} MB''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Processing with pdfminer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs\n",
    "\n",
    "- pdfminer Check\n",
    "    - Ref: https://euske.github.io/pdfminer/programming.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pwd': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience',\n",
       " 'datapath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data',\n",
       " 'infilename': '2003-Text Processing in Python.pdf',\n",
       " 'outfilename': '2003-Text Processing in Python.txt',\n",
       " 'mobydick': 'mobydick.txt',\n",
       " 'infilepath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data\\\\2003-Text Processing in Python.pdf',\n",
       " 'outfilepath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data\\\\2003-Text Processing in Python.txt',\n",
       " 'mobydickpath': 'C:\\\\Users\\\\hpzju\\\\Desktop\\\\MyKB\\\\DataScience\\\\data\\\\mobydick.txt'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "envs = {\n",
    "    'pwd': os.getcwd(),\n",
    "    'datapath': os.path.join(os.getcwd(), 'data'),\n",
    "    'infilename': r'2003-Text Processing in Python.pdf',\n",
    "    'outfilename': r'2003-Text Processing in Python.txt',\n",
    "    'mobydick': r'mobydick.txt'\n",
    "}\n",
    "envs['infilepath'] =  os.path.join(envs['datapath'], envs['infilename'])\n",
    "envs['outfilepath'] = os.path.join(envs['datapath'], envs['outfilename'])\n",
    "envs['mobydickpath'] = os.path.join(envs['datapath'], envs['mobydick'])\n",
    "\n",
    "envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check file and filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizeof \"2003-Text Processing in Python.pdf\": 1.51 MB\n",
      "sizeof \"2003-Text Processing in Python.txt\": 4.67e-05 MB\n"
     ]
    }
   ],
   "source": [
    "inputfile = envs['infilepath']\n",
    "outputfile = envs['outfilepath']\n",
    "with open(inputfile, 'br') as ifo, open(outputfile, 'w+') as ofo:\n",
    "    indata = ifo.read()\n",
    "    outdata = ofo.read()\n",
    "    print(f'''sizeof \"{envs['infilename']}\": {sys.getsizeof(indata)/1024/1024:.3} MB''')\n",
    "    print(f'''sizeof \"{envs['outfilename']}\": {sys.getsizeof(outdata)/1024/1024:.3} MB''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing PDF to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams, LTTextBoxHorizontal\n",
    "from pdfminer.converter import PDFPageAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with total 504 pages \n"
     ]
    }
   ],
   "source": [
    "# Open a PDF file.\n",
    "with open(inputfile, 'br') as ifo, open(outputfile, 'w') as ofo:\n",
    "    # Create a PDF parser object associated with the file object.\n",
    "    parser = PDFParser(ifo)\n",
    "    \n",
    "    # Create a PDF document object that stores the document structure.\n",
    "    # Supply the password for initialization.\n",
    "    document = PDFDocument(parser, password=\"\")\n",
    "    \n",
    "    # Check if the document allows text extraction. If not, abort.\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "\n",
    "    # Create a PDF resource manager object that stores shared resources.\n",
    "    rcmgr = PDFResourceManager()\n",
    "\n",
    "    # Create a PDF device object.\n",
    "    lap = LAParams()\n",
    "    device = PDFPageAggregator(rcmgr, laparams=lap)\n",
    "\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rcmgr, device)\n",
    "    \n",
    "    # Process each page contained in the document.\n",
    "    for num, page in enumerate(PDFPage.create_pages(document)):\n",
    "        interpreter.process_page(page)\n",
    "        layout = device.get_result()\n",
    "        for element in layout:\n",
    "            if isinstance(element, LTTextBoxHorizontal):\n",
    "                ofo.write(element.get_text())\n",
    "     \n",
    "    print(f\"Done with total {num} pages \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TXT Processing with re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "processingfile = envs['outfilepath']\n",
    "mobydickfile = envs['mobydickpath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 22333\n",
      "totoal words: 222663\n",
      "Most common TopN words:\n",
      "top 1:  ('the', 13972)\n",
      "top 2:  ('of', 6699)\n",
      "top 3:  ('and', 6144)\n",
      "top 4:  ('a', 4648)\n",
      "top 5:  ('to', 4635)\n",
      "top 6:  ('in', 3997)\n",
      "top 7:  ('that', 2994)\n",
      "top 8:  ('his', 2472)\n",
      "top 9:  ('it', 2222)\n",
      "top 10:  ('I', 2120)\n",
      "top 11:  ('s', 1807)\n",
      "top 12:  ('is', 1723)\n",
      "top 13:  ('with', 1709)\n",
      "top 14:  ('he', 1665)\n",
      "top 15:  ('was', 1635)\n",
      "top 16:  ('as', 1631)\n",
      "top 17:  ('all', 1484)\n",
      "top 18:  ('for', 1442)\n",
      "top 19:  ('this', 1318)\n",
      "top 20:  ('at', 1251)\n",
      "top 21:  ('by', 1165)\n",
      "top 22:  ('not', 1124)\n",
      "top 23:  ('but', 1117)\n",
      "top 24:  ('from', 1076)\n",
      "top 25:  ('him', 1060)\n",
      "top 26:  ('be', 1048)\n",
      "top 27:  ('on', 1027)\n",
      "top 28:  ('so', 919)\n",
      "top 29:  ('whale', 911)\n",
      "top 30:  ('one', 895)\n"
     ]
    }
   ],
   "source": [
    "word_stat = Counter()\n",
    "word_matcher = re.compile(r'(\\w+)')\n",
    "with open(mobydickfile, 'r') as fo:\n",
    "    for index, line in enumerate(fo.readlines()):\n",
    "        line=line.strip()\n",
    "        words = word_matcher.split(line)[1::2]\n",
    "        line_stat = Counter(words)\n",
    "        word_stat.update(line_stat)\n",
    "#         print(f'{index:>5}:\"{line}\":')\n",
    "#         print(f'\\t{line_stat}')\n",
    "#         if index > 100:\n",
    "#             break\n",
    "    print(f'total lines: {index+1}')\n",
    "    print(f'totoal words: {sum(word_stat.values())}')\n",
    "    print(f'Most common TopN words:')\n",
    "    for topn, word in enumerate(word_stat.most_common(30)):\n",
    "        print(f'top {topn+1}: ', word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools and Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)|(?P<NUM>\\d+)|(?P<OP>[+\\-*/])|(?P<EQ>[=])|(?P<WS>\\s+)\n",
      "re.compile('(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)|(?P<NUM>\\\\d+)|(?P<OP>[+\\\\-*/])|(?P<EQ>[=])|(?P<WS>\\\\s+)')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "token_pattern = {\n",
    "    'NAME': r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)',\n",
    "    'NUM': r'(?P<NUM>\\d+)', \n",
    "    'OP': r'(?P<OP>[+\\-*/])',\n",
    "    'EQ': r'(?P<EQ>[=])',\n",
    "    'WS': r'(?P<WS>\\s+)'\n",
    "}\n",
    "\n",
    "print('|'.join([token_pattern[key] for key in token_pattern]))\n",
    "\n",
    "pattern = re.compile('|'.join([token_pattern[key] for key in token_pattern]))\n",
    "\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'foo = 42+23-422*2/23'\n",
    "\n",
    "sc = pattern.scanner(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', ['Type','Value'])\n",
    "def generate_tokens(pat, textline):\n",
    "    scanner = pat.scanner(textline)\n",
    "    for m in iter(scanner.match, None):\n",
    "        yield Token(m.lastgroup, m.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(Type='NAME', Value='foo')\n",
      "Token(Type='WS', Value=' ')\n",
      "Token(Type='EQ', Value='=')\n",
      "Token(Type='WS', Value=' ')\n",
      "Token(Type='NUM', Value='42')\n",
      "Token(Type='OP', Value='+')\n",
      "Token(Type='NUM', Value='23')\n",
      "Token(Type='OP', Value='-')\n",
      "Token(Type='NUM', Value='422')\n",
      "Token(Type='OP', Value='*')\n",
      "Token(Type='NUM', Value='2')\n",
      "Token(Type='OP', Value='/')\n",
      "Token(Type='NUM', Value='23')\n"
     ]
    }
   ],
   "source": [
    "for tok in generate_tokens(pattern, line):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
