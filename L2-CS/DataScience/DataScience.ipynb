{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised Learning\n",
    "  - Regression\n",
    "    - Linear Regression\n",
    "      - low dimensional, ridge regression, lasso, greedy regression\n",
    "    - Nonpar Regression: \n",
    "      - kernel regression, local polynomials, additive, RKHS regression\n",
    "  - Classification\n",
    "    - Linear Classification: \n",
    "      - linear, logistic, SVM, sparse logistic\n",
    "    - Nonpar Classification: \n",
    "      - NN, naive Bayes, plug-in, kernelized SVM\n",
    "  - Conformal Prediction\n",
    "  - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsupervised Learning\n",
    "  - Clustering: \n",
    "    - k-means, mixtures, single-linkage, density clustering, spectral clustering\n",
    "  - Nonpar Density Estimation\n",
    "  - Measures of Dependence\n",
    "  - Graphical Models: \n",
    "    - correlation graphs, partial correlation graphs, cond. indep. graphs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "id: DataScience\n",
    "title: DataScience\n",
    "sidebar_label: DataScience\n",
    "---\n",
    "\n",
    "## History and Evolve\n",
    "\n",
    "---\n",
    "\n",
    "## Theories\n",
    "\n",
    "### CAP\n",
    "\n",
    "- Consistancy\n",
    "- Availability\n",
    "- Partition Tolerance\n",
    "\n",
    "- CAP: It is impossible for a distributed computer system to simultaneously provide all three of the following guarantees: Consistency, Availability, and Partition Tolerance\n",
    "\n",
    "### ACID\n",
    "\n",
    "- Atomicity\n",
    "- Consistency\n",
    "- Isolation\n",
    "- Durability\n",
    "\n",
    "### Categories\n",
    "\n",
    "- IMQAV model\n",
    "  - Ingest\n",
    "  - Model\n",
    "  - Query\n",
    "    - SQL:\n",
    "      - RDBMS: MySQL, PostgreSQL, SQLServer, Oracle\n",
    "      - Store: Row + Column\n",
    "      - Schema on write\n",
    "    - NoSQL:\n",
    "      - DBMS: MongoDB, Cassandra, Redis, Neo4j, HBase\n",
    "      - Store:\n",
    "        - Document\n",
    "        - Key-Value\n",
    "        - Graph-based\n",
    "      - Schema on read\n",
    "  - Analyze\n",
    "    - Descriptive Analysis\n",
    "    - Exploratory Analysis\n",
    "    - Predictive Analysis\n",
    "    - Mechanistic Analysis\n",
    "    - Inferential Analysis\n",
    "    - Causal Analysis\n",
    "  - Visulize\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "### Cyber-Physical-Social Systems\n",
    "\n",
    "## Technologies\n",
    "\n",
    "### NoSQL\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "- Categories\n",
    "\n",
    "  - Key-Value Store\n",
    "  - Document Store\n",
    "  - Graph Store\n",
    "  - Column Store\n",
    "\n",
    "- Consistency Models\n",
    "\n",
    "  - not ACID\n",
    "  - BASE\n",
    "    - Basic Availability\n",
    "    - Soft-state\n",
    "    - Eventual consistency\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Big Data\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- 5V Features:\n",
    "\n",
    "  - Volume\n",
    "    - huge amount of data\n",
    "  - Variety\n",
    "    - variety of source of data\n",
    "    - variety of format of data\n",
    "  - Velocity\n",
    "    - high speed of accumulation of data\n",
    "  - Veracity\n",
    "    - inconsistancy and uncertainty in data\n",
    "    - Quality\n",
    "    - Validation\n",
    "  - Value\n",
    "    - useful of data\n",
    "\n",
    "- Data Categories:\n",
    "\n",
    "  - Structured\n",
    "    - tables in DB, excel table\n",
    "    - sensor data generated by Machine\n",
    "    - Weblogs generated by Machine\n",
    "  - Semi-structured\n",
    "    - webpage\n",
    "    - JSON\n",
    "    - XML\n",
    "  - Unstructured\n",
    "    - PDF\n",
    "    - txt\n",
    "    - images\n",
    "    - videos\n",
    "\n",
    "- Data Sources:\n",
    "\n",
    "  - Web Logs\n",
    "  - IoT Sensors\n",
    "  - Social Network\n",
    "  - Webs 2.0/3.0\n",
    "  - Scientific Data\n",
    "  - By Vertical Industials:\n",
    "    - Healthcare\n",
    "      - ICU Monitorying\n",
    "      - Epidemic early warning\n",
    "    - Transportation\n",
    "      - traffic congestion\n",
    "      - logistic optimization\n",
    "    - Telecommunication\n",
    "      - network ops\n",
    "      - geo-mapping\n",
    "    - IT\n",
    "      - cyber security\n",
    "      - system log analysis\n",
    "    - Retail\n",
    "      - realtime promotion\n",
    "      - timely analysis of inventory\n",
    "    - Fintech\n",
    "      - Fraud detection\n",
    "      - Audit trials\n",
    "      - Risk management\n",
    "      - Customer Insights\n",
    "      - Cybersecurity\n",
    "\n",
    "- Data Lifecycle:\n",
    "\n",
    "  - Business Case Development\n",
    "  - Identify Data\n",
    "  - Data filtering\n",
    "  - Data extracting\n",
    "  - Data aggregation\n",
    "  - Data analysis\n",
    "  - Data visualize\n",
    "  - Business Case Validation\n",
    "\n",
    "- Analytic Categories\n",
    "\n",
    "  - Descriptive Analytic: what happened\n",
    "  - Diagnostic Analytic: why it happened\n",
    "  - Predictive Analytic: what will happen\n",
    "  - Prescriptive Analytic: what's the solution\n",
    "\n",
    "- Technologies Categories\n",
    "\n",
    "  - Flow Perspective\n",
    "\n",
    "    - data integration\n",
    "      - Online\n",
    "      - Offline\n",
    "    - data in transient\n",
    "      - Operational\n",
    "    - data in rest\n",
    "      - Analytic\n",
    "      - Real-time interactive\n",
    "      - Batch-Oriented Analytic\n",
    "\n",
    "  - Stack Perspective\n",
    "\n",
    "    - Data Storage and Management\n",
    "    - Data Cleaning\n",
    "    - Data Mining\n",
    "    - Data Visualization\n",
    "    - Data Reporting\n",
    "    - Data Ingestion\n",
    "    - Data Analysis\n",
    "    - Data Acquigisition\n",
    "\n",
    "### Processing Framework\n",
    "\n",
    "- Categories:\n",
    "\n",
    "  ![Alt](/img/DS-Hadoop-ComputingFramwork.png \"Computing Framwork\")\n",
    "\n",
    "  - General-purpose processing frameworks\n",
    "  - Abstraction frameworks\n",
    "  - SQL frameworks\n",
    "  - Graph processing frameworks\n",
    "  - Machine learning frameworks\n",
    "  - Real-time/streaming frameworks\n",
    "  - Batch Process Framework: bounded, persistent, large\n",
    "    - MapReduce\n",
    "      - Input -> Split -> Map -> Reduce -> Output\n",
    "  - Steam Process Framework: unbounded\n",
    "    - Storm\n",
    "      - real-time stream processing.\n",
    "    - Samza\n",
    "      - near real-time stream processing.\n",
    "  - Hybrid Process Framework\n",
    "    - Spark\n",
    "      - Spark SQL\n",
    "      - Spark Streaming\n",
    "      - Spark MLlib\n",
    "      - GraphX\n",
    "    - Flink\n",
    "\n",
    "### Hadoop\n",
    "\n",
    "#### Hadoop Overview\n",
    "\n",
    "- Hadoop\n",
    "\n",
    "  - a big data processing framework running on commodity hardware easily.\n",
    "  - provides data storage, resources management, data processing and etc. capabilities\n",
    "  - written in Java\n",
    "  - governed by Apache Software Fundatiaon\n",
    "\n",
    "- Vendors\n",
    "  - Amazon, Microsoft, AliCloud\n",
    "  - Huawei, IBM, HPE\n",
    "  - Cloudera, Hortonworks, MapR, MapReduce\n",
    "\n",
    "#### Hadoop Architecture\n",
    "\n",
    "- Intro\n",
    "\n",
    "  - Data Storage\n",
    "  - Cluster Management\n",
    "  - Data Processing\n",
    "\n",
    "- HDFS\n",
    "\n",
    "  - Features\n",
    "    - cost-effective: build on commodity hardware\n",
    "    - scale-out: distributed system for large volume datastore\n",
    "    - falt-tolerance: n-copies\n",
    "  - Building Blocks\n",
    "    - Client\n",
    "    - NameNode\n",
    "    - Secondary NameNode\n",
    "    - DataNode\n",
    "    - Data Block: 128MB by default\n",
    "    - Metadata\n",
    "      - editlog\n",
    "      - fsimage\n",
    "  - Architecture\n",
    "    ![Alt](/img/DS-Hadoop-Architecture-HDFS.png \"HDFS Architecture\")\n",
    "    - Primary-Secondary namenode\n",
    "    - master-worker model\n",
    "    - n-copies redundancy: 3 by default\n",
    "    - rack awareness HA\n",
    "  - User Case\n",
    "    - not for large number of small files\n",
    "    - WORM: write once, read many times\n",
    "\n",
    "- YARN\n",
    "\n",
    "  - scalability, compatibility, resouce utilization, multitanants\n",
    "  - Building Block\n",
    "    - Client\n",
    "    - ResourceManager\n",
    "      - negotiate resources required by app master\n",
    "      - Scheduler\n",
    "      - Application Manager\n",
    "    - NodeManager\n",
    "      - approve resources required by resource manager\n",
    "      - Container\n",
    "        - resources abstration on ram, cpu, ios\n",
    "      - App Master\n",
    "        - get task execution done\n",
    "  - Architecture\n",
    "    ![Alt](/img/DS-Hadoop-Architecture-YARN.png \"YARN Architecture\")\n",
    "  - User Case\n",
    "\n",
    "- Oozie\n",
    "\n",
    "  - a scheduler system for managing hadoop jobs in a distributed environment.\n",
    "  - Building Block\n",
    "    - Jobs\n",
    "      - Oozie workflow jobs\n",
    "      - Oozie coordinator jobs\n",
    "      - Oozie bundle\n",
    "  - Architecture\n",
    "  - User Case\n",
    "\n",
    "- HBase\n",
    "\n",
    "  - NoSQL, Non-Relational, Distributed column-oriented Database system works on HDFS\n",
    "    - Scalable\n",
    "    - HA\n",
    "  - Building Blocks\n",
    "    - client\n",
    "    - table, row, column famility, column, k-v pair\n",
    "    - Cell\n",
    "    - HBase Data Model\n",
    "      ![Alt](/img/DS-Hadoop-Architecture-HBase-DataModel.png \"HBase DataModel\")\n",
    "  - Architecture\n",
    "    ![Alt](/img/DS-Hadoop-Architecture-HBase.png \"HBase Architecture\")\n",
    "    - HMaster\n",
    "    - RegionServer\n",
    "      - Region\n",
    "        - BlockCache\n",
    "        - Memstore\n",
    "      - HDFS\n",
    "        - HFile\n",
    "        - Index\n",
    "    - Zookeeper\n",
    "    - HBase Shell\n",
    "  - User Case\n",
    "\n",
    "    - real-time random r/w data service\n",
    "    - sparse tables processing\n",
    "    - structured and unstructured data processing\n",
    "    - no transaction integrity\n",
    "    - no referential integrity\n",
    "\n",
    "- MapReduce\n",
    "\n",
    "  - a batch processing framework for large dataset\n",
    "  - Building Blocks\n",
    "    - Map Tasks\n",
    "      - K-V pairs\n",
    "      - split dataset depends on business logic\n",
    "    - Reduce Tasks\n",
    "      - K-V pairs\n",
    "      - shuffle/aggregate/sort/summary and etc.\n",
    "    - Data Flow\n",
    "      ![Alt](/img/DS-Hadoop-ComputingFramwork-MapReduce-Dataflow.png \"MapReduce Data Flow\")\n",
    "  - Architecture\n",
    "    ![Alt](/img/DS-Hadoop-ComputingFramwork-MapReduce.png \"MapReduce Architecture\")\n",
    "    - Input-Map-Reduce-Output Model\n",
    "      - Input from HDFS\n",
    "      - Mapper Class\n",
    "        - Init Inputs\n",
    "        - Mapping\n",
    "        - Shuffling/Sorting\n",
    "      - Reducer Class\n",
    "        - Searching\n",
    "        - Reducing\n",
    "      - Output to HDFS\n",
    "    - Job Scheduling(Yarn Resource Manager)\n",
    "      - FIFO Scheduler\n",
    "      - Capacity Scheduler\n",
    "      - Fair Scheduler\n",
    "  - User Case\n",
    "    - Batch processing framework\n",
    "\n",
    "- Hive\n",
    "\n",
    "  - data warehouse infrustructure to process structured data with HQL on top of HDFS.\n",
    "  - Building Block\n",
    "    - Client\n",
    "      - Thrift\n",
    "      - JDBC\n",
    "      - ODBC\n",
    "    - Hive Server\n",
    "    - Hive GUI\n",
    "    - Hive CLI\n",
    "    - Hive Driver\n",
    "      - Compiler\n",
    "      - Optimizer\n",
    "      - Executor\n",
    "    - Metastore\n",
    "    - Table\n",
    "    - Partition\n",
    "    - Bucket\n",
    "  - Architecture\n",
    "    ![Alt](/img/DS-Hadoop-Architecture-Hive.png \"Hive Architecture\")\n",
    "    - Hive Data Model\n",
    "      - Data Types\n",
    "    - Hive Dataflow\n",
    "      ![Alt](/img/DS-Hadoop-Architecture-Hive-Dataflow.png \"Hive Architecture\")\n",
    "  - User Case\n",
    "    - EDW\n",
    "\n",
    "- Pig\n",
    "\n",
    "  - an abstract layer over mapreduce with pig latin and pig engine to process data\n",
    "  - Building Block\n",
    "    - Pig Shell\n",
    "      - Grunt\n",
    "    - Pig Server\n",
    "    - Parser\n",
    "    - Optimizer\n",
    "    - Compiler\n",
    "    - Execution Engine\n",
    "  - Architecture\n",
    "    - pig latin scripts tranlated into Map-Reduce tasks for execution.\n",
    "    - Data Model\n",
    "      - atom: int, long, float, double, chararray, datetime, boolean, bytearray\n",
    "      - atom -> field -> tuple -> bag -> relation\n",
    "      - map\n",
    "    - Operators\n",
    "      - LOAD, STORE, FILTER, DISTINCT, FOREACH ... GENERATE, STREAM, DUMP\n",
    "      - JOIN, COGROUP, GROUP, CROSS, ORDER, LIMIT, UNION, SPLIT, DESCRIBE\n",
    "  - User Case\n",
    "\n",
    "    - structure and unstructure data processing\n",
    "\n",
    "- Spark\n",
    "\n",
    "  - In-Memory cluster processing framework,\n",
    "  - Building Block\n",
    "    - Spark SQL\n",
    "    - Spark Stream\n",
    "    - MLlib\n",
    "    - GraphX\n",
    "    - SparkR\n",
    "    - Spark Shell\n",
    "      - scala\n",
    "  - Architecture\n",
    "    - APIs\n",
    "      - R, SQL, Python, Scala, Java\n",
    "    - RDD\n",
    "      - Resilient Distributed Dataset\n",
    "      - Operation:\n",
    "        - Transformation\n",
    "        - Action\n",
    "  - User Case\n",
    "\n",
    "- Sqoop\n",
    "\n",
    "#### Hadoop Ecosystem\n",
    "\n",
    "- Intro\n",
    "  ![Alt](/img/DS-Hadoop-Ecosystem-1.png \"Hadoop Ecosystem\")\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Schema Design Principle\n",
    "\n",
    "- understand your DBMS features and limitations\n",
    "- understand your APP and DATA patterns\n",
    "- Balance these two facets during data modeling desicion-making process\n",
    "\n",
    "---\n",
    "\n",
    "[1]: , \"empirical evidence, scientific theory, computational science, data science\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "id: Python4DS\n",
    "title: Python Data Science DevOps.\n",
    "sidebar_label: Data Science with Python\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Anaconda\n",
    "\n",
    "### Package Management\n",
    "\n",
    "- conda\n",
    "  - basics\n",
    "    - `conda --version`\n",
    "  - virtual env\n",
    "    - `conda info --envs`\n",
    "    - `onda create -n MYENV`\n",
    "    - `conda create -n MYENV --clone OLDENV`\n",
    "    - `conda create -n MYENV python=3.6.0`\n",
    "    - `conda activate MYENV`\n",
    "    - `conda deactivate MYENV`\n",
    "  - package management\n",
    "    - `conda list`\n",
    "    - `conda search PACKAGE`\n",
    "    - `conda update conda`\n",
    "    - `conda update anaconda`\n",
    "    - `conda update --all`\n",
    "    - `conda update PACKAGE`\n",
    "    - `conda install PACKAGE`\n",
    "    - `conda install PACKAGE=M.N.P`\n",
    "    - `conda remove PACKAGE`\n",
    "    - `conda build PACKAGE`\n",
    "  - config sources\n",
    "    - `conda config --show-source`\n",
    "    - `conda config --remove channels NOT_WANTED`\n",
    "  - set conda-forge\n",
    "    - `conda config --add channels conda-forge`\n",
    "    - `conda config --set channel_priority strict`\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Misc\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Anaconda Document](https://docs.anaconda.com/anaconda/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
